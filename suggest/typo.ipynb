{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee7ec42e-a230-419a-8f32-ffa4af07bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "db_path = 'deals_db.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d8a3204-2145-431a-9dfb-2eeaeb0907b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'\\b[a-zA-Z]\\b|\\b\\d+\\b', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ef284-fdc5-4b99-91a7-32ccb036e52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6d888dd-6d54-4ca9-bae8-8397a0dd73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"SELECT c.title || \" \" ||  c.title_general as document\n",
    "    FROM deals c\n",
    "\"\"\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6416ef0-c98d-432e-9ace-a532814f828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for row in rows:\n",
    "    if row[0]:\n",
    "        line = clean_text(row[0])\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5dfb9ee-88ce-4800-9964-db7006245117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunset tour by kayak on sebago lake maine sunset tour by kayak on sebago lake maine',\n",
       " 'quarter rotisserie chicken meal savor the flavor: quarter rotisserie chicken meal at texas rotisserie and grill- broadway(up to % off)']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "480ba8ba-2c39-4387-b6ec-99af1e18fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for line in corpus:\n",
    "    data = word_tokenize(line)\n",
    "    for word in data:\n",
    "    #filtered_data = [[word for word in sentence if len(word) > 2] for sentence in data]\n",
    "        if len(word) > 2:\n",
    "            tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b008289-f94e-43b1-a08c-989128be00f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71ba1f-9851-4aa4-8e77-7c5ee2d11a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = [' '.join(sentence) for sentence in tokens]\n",
    "documents = [' '.join(tokens)]\n",
    "\n",
    "# Inicializace TfidfVectorizer\n",
    "#tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Fit and transform data\n",
    "#tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "#trigrams = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Compute the mean TF-IDF scores\n",
    "#mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "\n",
    "#filtered_mean_tfidf = [(trigram, tfidf) for trigram, tfidf in zip(trigrams, mean_tfidf) if tfidf > 0.000001]\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "#output_file = 'models/mean_tfidf_unigram.csv'\n",
    "#with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#    f.write('unigram,tfidf\\n')\n",
    "#    for trigram, tfidf in filtered_mean_tfidf:\n",
    "#       f.write(f'{trigram},{tfidf}\\n')\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce26565-3132-4e88-9ab9-416ec4022aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unigram    51009\n",
       "tfidf      51009\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"models/mean_tfidf_unigram.csv\").sort_values([\"tfidf\"], ascending = False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3dc81f4d-85a5-4527-afb9-7506e6376e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('002oz57ml', 1.5954199494191315e-05),\n",
       " ('003oz11g', 1.5954199494191315e-05),\n",
       " ('004oz11g', 4.786259848257395e-05),\n",
       " ('007oz22g', 1.5954199494191315e-05),\n",
       " ('008oz24g', 1.5954199494191315e-05),\n",
       " ('010oz3g', 1.5954199494191315e-05),\n",
       " ('012oz35g', 1.5954199494191315e-05),\n",
       " ('015ct', 2.3931299241286974e-05),\n",
       " ('015mm', 2.3931299241286974e-05),\n",
       " ('016oz52ml', 1.5954199494191315e-05)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mean_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448298d8-1be9-4be3-8f4b-441f4b476e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"models/mean_tfidf_unigram.csv\").sort_values([\"tfidf\"], ascending = False)\n",
    "df.count()\n",
    "dictionary = df[\"unigram\"].tolist()\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1aad465a-f946-4501-830a-4b17d99fbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vol', 90)\n",
      "('message', 92)\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "dictionary = df[\"unigram\"].tolist()\n",
    "for w in [\"valvoln\", \"mesage\"]:\n",
    "    corrected_word = process.extractBests(w, dictionary)\n",
    "    print(corrected_word[0]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2e6fdaba-7b1b-4348-8b4e-1a2045f2892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zphilipp/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f616c883-367c-4cc6-b4cc-914fee401744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"models/mean_tfidf_unigram.csv\").sort_values([\"tfidf\"], ascending = False)\n",
    "df.count()\n",
    "dictionary = df[\"unigram\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51e0bbc6-c453-425f-af2c-e626edb46f3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Příklad použití\u001b[39;00m\n\u001b[1;32m     28\u001b[0m input_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmassage\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Překlep\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m corrected_word \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_typo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpravené slovo pro \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m je \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrected_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 25\u001b[0m, in \u001b[0;36mcorrect_typo\u001b[0;34m(input_word)\u001b[0m\n\u001b[1;32m     22\u001b[0m most_similar_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(similarities)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Vrátit slovo ze slovníku s nejvyšší podobností\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdictionary_words\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmost_similar_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def correct_typo(input_word):\n",
    "    # Tokenizace vstupního slova\n",
    "    input_words = [input_word]\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"models/mean_tfidf_unigram.csv\").sort_values([\"tfidf\"], ascending = False)\n",
    "    df.count()\n",
    "    dictionary = df[\"unigram\"].tolist()\n",
    "    # Vytvoření TF-IDF vektorizeru\n",
    "    vectorizer = TfidfVectorizer().fit(input_words + dictionary)\n",
    "    \n",
    "    # Vytvoření vektorů pro vstupní slovo a slova ve slovníku\n",
    "    input_vector = vectorizer.transform(input_words)\n",
    "    \n",
    "\n",
    "    dictionary_vectors = vectorizer.transform(dictionary)\n",
    "\n",
    "    # Výpočet cosine similarity\n",
    "    similarities = cosine_similarity(input_vector, dictionary_vectors)\n",
    "\n",
    "    # Najděte index slova s nejvyšší podobností\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    # Vrátit slovo ze slovníku s nejvyšší podobností\n",
    "    return dictionary_words[most_similar_index]\n",
    "\n",
    "# Příklad použití\n",
    "input_word = \"massage\"  # Překlep\n",
    "\n",
    "corrected_word = correct_typo(input_word)\n",
    "print(f\"Opravené slovo pro '{input_word}' je '{corrected_word}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870d935-8953-4b46-be66-b223d3bc0372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
